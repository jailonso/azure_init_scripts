%python 

dbutils.fs.put("dbfs:/databricks/init/datadog-install-driver-only.sh","""
#!/bin/bash

echo "Running on the driver? $DB_IS_DRIVER"
echo "Driver ip: $DB_DRIVER_IP"

cat <<EOF >> /tmp/start_datadog.sh
#!/bin/bash
DD_API_KEY="xxx"
DD_SITE="datadoghq.eu"
DD_TAGS="env:edp"
DATADOG_ROOT_PATH="/etc/datadog-agent"

if [ \$DB_IS_DRIVER ]; then
  echo "On the driver. Installing Datadog ..."
  
  # install the Datadog agent
  DD_AGENT_MAJOR_VERSION=7 DD_API_KEY=$DD_API_KEY DD_TAGS="$DD_TAGS" DD_SITE="$DD_SITE" bash -c "\$(curl -L https://raw.githubusercontent.com/DataDog/datadog-agent/master/cmd/agent/install_script.sh)"
  
  
  # WAITING UNTIL MASTER PARAMS ARE LOADED, THEN GRABBING IP AND PORT
  while [ -z \$gotparams ]; do
    if [ -e "/tmp/master-params" ]; then
      DB_DRIVER_PORT=\$(cat /tmp/master-params | cut -d' ' -f2)
      gotparams=TRUE
    fi
    sleep 2
  done

  # Get current hostname
  current=\$(hostname -I | xargs)  
  
  # Enable logs, process and network collection
  echo -e "\nEnabling logs, network and process collection..."
  cat >> $DATADOG_ROOT_PATH/datadog.yaml <<EOL
  logs_enabled: true
  system_probe_config:
    enabled: "true"
  process_config:
    enabled: "true"
  EOL
 
  sudo -u dd-agent cp $DATADOG_ROOT_PATH/system-probe.yaml.example $DATADOG_ROOT_PATH/system-probe.yaml
  
  # Enable spark integration for streaming spark metrics
  echo -e "\nEnabling spark integration..."
  sudo -u dd-agent -- datadog-agent integration install datadog-spark==1.11.3
  cat > $DATADOG_ROOT_PATH/conf.d/spark.yaml << EOL
  instances:
      - resourcemanager_uri: http://$DB_DRIVER_IP:$DB_DRIVER_PORT
        spark_cluster_mode: spark_standalone_mode
        cluster_name: $current
  EOL
  
  # Restart datadog agent
  sudo service datadog-agent-sysprobe start
  sudo service datadog-agent restart

fi
EOF

# CLEANING UP
if [ \$DB_IS_DRIVER ]; then
  chmod a+x /tmp/start_datadog.sh
  /tmp/start_datadog.sh >> /tmp/datadog_start.log 2>&1 & disown
fi
""", True)
